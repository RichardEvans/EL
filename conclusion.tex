\section{Conclusion}\label{conclusion}


\QUOTATION{As long as a branch of science offers an abundance of
  problems, so long it is alive; a lack of problems foreshadows
  extinction or the cessation of independent development. \textsc{D. Hilbert.}}

\subsection{Open problems} In the spirit of Hilbert's quote, I'm including this
section to encourage us to think about interesting open problems,
ideally of varying degrees of difficulty. What I recommend to avoid
are vague problems, there are enough of those already. I'm looking for
clearcut questions, i.e. we know when we've succeeded in solving them.
A good question would ideally come with a plausible story why solving
the problem is important.  Here are some examples to get the ball
rolling.

\begin{enumerate}

\item Is there a reasonable Curry-Howard correspondence for eremic
  logic?

\item\label{conclusion:openProblems:2}  Develop a theory of SAT solving based on eremic rather than
  propositional logic.

\item Once (\ref{conclusion:openProblems:2}) works, develop efficient
  solvers for eremic satisfiability.

\item\label{conclusion:openProblems:4} Develop machine-oriented proof-rules that relate to eremic logic
  in the way that unification/resolution relate to first-order logic.

\item Once (\ref{conclusion:openProblems:4}) works, develop a
  programming language that relates to eremic logic in the same way
  Prolog relates to classical logic.

\item Develop and axiomatise an eremic $\mu$-calulus, following
  Kozen's modal-$\mu$-calculus.

\end{enumerate}

\subsection{Related work}

Here is a brief list of things that I suggest we compare EL with.

\begin{itemize}

\item Linear logic \cite{GirardJY:linlog,GirardJY:protyp}. Especially
  interesting in the context of eremic logic is the additive
  conjunction $A \& B$ which has been interpreted
  \cite{AbramskyS:comintoll} as an external choice operation in the
  terminology of CSP \cite{HoareC:comseq}. External, because the
  choice is offered to the environment. This interpretation has been
  influential in the study of types for process calculus,
  e.g.~\cite{HondaK:unitypsfsifLONG,TakeuchiK:intbaslaits,HondaK:lanpriatdfscbp}. \textbf{check
    if Pierce, Kobayashi, Turner also already do this.}

  Simplifying a great deal, a key difference between $!A$ and additive
  conjunction $A \& B$ is that the individual actions in $!A$ have no
  continuation, while they do with $A \& B$: $!{\mathsf{left},
    \mathsf{right}}$ says that at this point the only available
  actions are $\mathsf{left}$ and $\mathsf{right}$, while $A \& B$
  says that at this point the only available actions are
  $\mathsf{left}$ and $\mathsf{right}$, and if we do $\mathsf{left}$,
  then $A$ holds, while doing $\mathsf{right}$ guarantees $B$.

  So both, eremic and linear logic offer an operator that restricts
  the available options. How are they related? Linear logic has an
  explicit linear negation $(\cdot)^{\bot}$ which, unlike classical
  negation, is constructive. In constrast, eremic logic defines
  negation from $!A$. Can these two perspectives be frutifully
  reconciled?

\item Process calculi traditionally 
  have sums which in their most general form are:
  \[
     \sum_{i \in I} P_i
  \]
  But input-guarded sums are much better behaved (and strictly less
  expressive):
  \[
     \sum_{i \in I} x_{i}(v_i)P_i
  \]
  and they are even better behaved if they are all using the same
  input channel and have only finitely many alternatives:
  \[
     \sum_{i = 1}^n x(v)P_i
  \]
  Simplifying a great deal, this can be seen as a proof for linear
  logic's additive conjunction
  \[
     \&_{i = 1}^n x(v)A_i
  \]
  provided each $P_i$ is a proof of $A_i$.  As with linear logic's
  additive conjunction, sums in process calculi have continuations.

\end{itemize}
